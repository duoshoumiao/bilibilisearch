import os
import json
import re
import time
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from urllib.parse import quote

from hoshino import Service, priv
from hoshino.typing import CQEvent
import aiohttp

# ä¸»æœåŠ¡å®šä¹‰
sv = Service('bç«™è§†é¢‘æœç´¢', enable_on_default=True, help_='æœç´¢Bç«™è§†é¢‘\nä½¿ç”¨æ–¹æ³•ï¼š\n1. æŸ¥è§†é¢‘ [å…³é”®è¯] - æœç´¢Bç«™è§†é¢‘\n2. è§†é¢‘å…³æ³¨ [è§†é¢‘é“¾æ¥] - é€šè¿‡è§†é¢‘é“¾æ¥å…³æ³¨UPä¸»\n3. å–å…³up [UPä¸»åç§°] - å–æ¶ˆç›‘æ§\n4. æŸ¥çœ‹å…³æ³¨ - æŸ¥çœ‹å½“å‰ç›‘æ§åˆ—è¡¨')

# é…ç½®é¡¹
MAX_RESULTS = 5
UP_WATCH_INTERVAL = 1  # ç›‘æ§é—´éš”(åˆ†é’Ÿ)
CACHE_EXPIRE_MINUTES = 3
search_cache = {}

# JSONå­˜å‚¨æ–‡ä»¶è·¯å¾„
WATCH_JSON_PATH = Path(__file__).parent / 'data' / 'bili_watch.json'
os.makedirs(WATCH_JSON_PATH.parent, exist_ok=True)

class UpWatchStorage:
    def __init__(self):
        self._data = {}
        self.name_index = {}  # åç§°å°å†™ç´¢å¼•
        self._load_and_migrate()
        sv.logger.info("UPä¸»ç›‘æ§å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_and_migrate(self):
        """åŠ è½½å¹¶è‡ªåŠ¨è¿ç§»æ—§æ ¼å¼æ•°æ®"""
        try:
            if WATCH_JSON_PATH.exists():
                with open(WATCH_JSON_PATH, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                    sv.logger.info(f"ä»æ–‡ä»¶åŠ è½½ç›‘æ§æ•°æ®ï¼Œå…± {sum(len(v) for v in old_data.values())} æ¡è®°å½•")
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ—§æ ¼å¼(åŒ…å«æ•°å­—UIDé”®)
                    is_old_format = any(
                        any(k.isdigit() for k in ups.keys())
                        for ups in old_data.values()
                    )
                    
                    if is_old_format:
                        sv.logger.info("æ£€æµ‹åˆ°æ—§æ ¼å¼æ•°æ®ï¼Œå¼€å§‹è‡ªåŠ¨è¿ç§»...")
                        self._migrate_from_old_format(old_data)
                    else:
                        self._data = old_data
                        # é‡å»ºåç§°ç´¢å¼•
                        for group_id, ups in old_data.items():
                            for up_name in ups.keys():
                                self.name_index[up_name.lower()] = (group_id, up_name)
                        sv.logger.info("æ•°æ®åŠ è½½å®Œæˆï¼Œæ— éœ€è¿ç§»")
                        
        except Exception as e:
            sv.logger.error(f"åŠ è½½ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}")
            self._data = {}
    
    def _migrate_from_old_format(self, old_data):
        """ä»æ—§æ ¼å¼è¿ç§»æ•°æ®"""
        migrated_count = 0
        for group_id, ups in old_data.items():
            self._data[group_id] = {}
            for uid, info in ups.items():
                try:
                    up_name = info['up_name']
                    # ç¡®ä¿åç§°å”¯ä¸€æ€§
                    if up_name.lower() in self.name_index:
                        sv.logger.warning(f"å‘ç°é‡å¤UPä¸»åç§°: {up_name}ï¼Œæ·»åŠ éšæœºåç¼€")
                        up_name = f"{up_name}_{uid[-4:]}"
                    
                    self._data[group_id][up_name] = {
                        'last_check': info['last_check'],
                        'last_vid': info['last_vid']
                    }
                    self.name_index[up_name.lower()] = (group_id, up_name)
                    migrated_count += 1
                except Exception as e:
                    sv.logger.error(f"è¿ç§»UPä¸» {uid} å¤±è´¥: {str(e)}")
        
        sv.logger.info(f"æ•°æ®è¿ç§»å®Œæˆï¼Œå…±è¿ç§» {migrated_count} æ¡è®°å½•")
        self.save()  # ç«‹å³ä¿å­˜æ–°æ ¼å¼
    
    def save(self):
        try:
            with open(WATCH_JSON_PATH, 'w', encoding='utf-8') as f:
                json.dump(self._data, f, ensure_ascii=False, indent=2)
            sv.logger.info("ç›‘æ§æ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            sv.logger.error(f"ä¿å­˜ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}")
    
    def add_watch(self, group_id: int, up_name: str, last_vid: str = None):
        group_id = str(group_id)
        if group_id not in self._data:
            self._data[group_id] = {}
        
        # æ ‡å‡†åŒ–åç§°å­˜å‚¨
        self._data[group_id][up_name] = {
            'last_check': datetime.now().isoformat(),
            'last_vid': last_vid
        }
        self.name_index[up_name.lower()] = (group_id, up_name)
        self.save()
        sv.logger.info(f"å·²æ·»åŠ ç›‘æ§: ç¾¤{group_id} -> UPä¸»{up_name}")
    
    def remove_watch(self, group_id: int, up_name: str) -> bool:
        group_id = str(group_id)
        if group_id in self._data and up_name in self._data[group_id]:
            del self._data[group_id][up_name]
            if up_name.lower() in self.name_index:
                del self.name_index[up_name.lower()]
            if not self._data[group_id]:
                del self._data[group_id]
            self.save()
            sv.logger.info(f"å·²ç§»é™¤ç›‘æ§: ç¾¤{group_id} -> UPä¸»{up_name}")
            return True
        sv.logger.warning(f"ç§»é™¤ç›‘æ§å¤±è´¥: ç¾¤{group_id} æœªç›‘æ§ UPä¸»{up_name}")
        return False
    
    def get_group_watches(self, group_id: int) -> Dict[str, Dict[str, Any]]:
        group_id = str(group_id)
        return self._data.get(group_id, {})
    
    def get_all_watches(self) -> Dict[str, Any]:
        return self._data
    
    def update_last_video(self, group_id: int, up_name: str, last_vid: str):
        group_id = str(group_id)
        if group_id in self._data and up_name in self._data[group_id]:
            self._data[group_id][up_name].update({
                'last_vid': last_vid,
                'last_check': datetime.now().isoformat()
            })
            self.save()
            sv.logger.info(f"æ›´æ–°è§†é¢‘è®°å½•: ç¾¤{group_id} -> UPä¸»{up_name} -> BV{last_vid}")
    
    def find_up_by_name(self, name: str) -> Optional[Tuple[str, str]]:
        """é€šè¿‡åç§°æŸ¥æ‰¾(ä¸åŒºåˆ†å¤§å°å†™)"""
        return self.name_index.get(normalize_name(name))

# å…¨å±€å­˜å‚¨å®ä¾‹
watch_storage = UpWatchStorage()

def normalize_name(name: str) -> str:
    """æ ‡å‡†åŒ–åç§°(å»å‰åç©ºæ ¼/å°å†™)"""
    return name.strip().lower()

async def get_video_info(bvid: str) -> Optional[Dict]:
    """è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': f'https://www.bilibili.com/video/{bvid}'
    }
    url = f'https://api.bilibili.com/x/web-interface/view?bvid={bvid}'
    
    async with aiohttp.ClientSession() as session:
        try:
            sv.logger.info(f"è·å–è§†é¢‘ä¿¡æ¯: BV{bvid}")
            async with session.get(url, headers=headers, timeout=10) as resp:
                if resp.status != 200:
                    sv.logger.error(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: HTTP {resp.status}")
                    return None
                data = await resp.json()
                if data.get('code') == 0:
                    sv.logger.info(f"æˆåŠŸè·å–è§†é¢‘ä¿¡æ¯: {data['data']['title']}")
                    return data['data']
                else:
                    sv.logger.error(f"è§†é¢‘APIè¿”å›é”™è¯¯: {data.get('message')}")
        except Exception as e:
            sv.logger.error(f"è·å–è§†é¢‘ä¿¡æ¯å¼‚å¸¸: {str(e)}")
    return None

async def get_bilibili_search(keyword: str, search_type: str = "video") -> list:
    cache_key = f"{search_type}:{normalize_name(keyword)}"
    if cache_key in search_cache:
        cached_data, timestamp = search_cache[cache_key]
        if datetime.now() - timestamp < timedelta(minutes=CACHE_EXPIRE_MINUTES):
            return cached_data[:MAX_RESULTS]

    params = {
        'search_type': 'video',
        'keyword': keyword,
        'order': 'pubdate' if search_type == "up" else 'totalrank',
        'ps': MAX_RESULTS,
        'platform': 'web'
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': 'https://www.bilibili.com/',
        'Origin': 'https://www.bilibili.com',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'no-cache',
        'Cookie': 'buvid3=XXXXXX;'  # æ·»åŠ å¿…è¦çš„cookie
    }

    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(
                'https://api.bilibili.com/x/web-interface/search/type',
                params=params,
                headers=headers,
                timeout=10
            ) as resp:
                if resp.status != 200:
                    sv.logger.error(f"æœç´¢è¯·æ±‚å¤±è´¥: HTTP {resp.status}")
                    return []
                data = await resp.json()
                if data.get('code') == 0:
                    results = data['data'].get('result', [])[:MAX_RESULTS]
                    if search_type == "up":
                        # ç¡®ä¿ç»“æœä¸­åŒ…å«ä½œè€…åå­—æ®µ
                        results = [v for v in results if 'author' in v]
                    search_cache[cache_key] = (results, datetime.now())
                    return results
        except Exception as e:
            sv.logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        return []

async def safe_send(bot, ev, message):
    """å®‰å…¨å‘é€æ¶ˆæ¯"""
    try:
        if not message:
            return
            
        if isinstance(message, list):
            message = '\n'.join(message)
            
        await bot.send(ev, message)
    except Exception as e:
        sv.logger.error(f'å‘é€æ¶ˆæ¯å¤±è´¥: {str(e)}')

@sv.on_prefix('è§†é¢‘å…³æ³¨')
async def watch_by_video(bot, ev: CQEvent):
    """é€šè¿‡è§†é¢‘é“¾æ¥å…³æ³¨UPä¸»"""
    video_url = ev.message.extract_plain_text().strip()
    if not video_url:
        await bot.send(ev, 'è¯·è¾“å…¥è§†é¢‘é“¾æ¥ï¼Œä¾‹å¦‚ï¼šè§†é¢‘å…³æ³¨ https://www.bilibili.com/video/BV1B73kzcE1e')
        return
    
    # å¢å¼ºç‰ˆBVå·æå–é€»è¾‘
    bvid = None
    patterns = [
        r'bilibili\.com/video/(BV[0-9A-Za-z]+)',  # æ ‡å‡†é“¾æ¥
        r'b23\.tv/(BV[0-9A-Za-z]+)',             # çŸ­é“¾æ¥
        r'(BV[0-9A-Za-z]+)',                      # çº¯BVå·
        r'bilibili\.com/video/av\d+\?.*bv=(BV[0-9A-Za-z]+)',  # æ—§ç‰ˆavå·é“¾æ¥å¸¦bvå‚æ•°
        r'video/(BV[0-9A-Za-z]+)/?'              # ç®€åŒ–é“¾æ¥
    ]
    
    for pattern in patterns:
        match = re.search(pattern, video_url)
        if match:
            bvid = match.group(1)
            break
    
    if not bvid:
        await bot.send(ev, 'æ— æ³•ä»é“¾æ¥ä¸­è¯†åˆ«è§†é¢‘BVå·ï¼Œè¯·ç¡®è®¤é“¾æ¥æ ¼å¼æ­£ç¡®\n'
                          'æ”¯æŒçš„æ ¼å¼ç¤ºä¾‹:\n'
                          '1. https://www.bilibili.com/video/BV1B73kzcE1e\n'
                          '2. https://b23.tv/BV1B73kzcE1e\n'
                          '3. BV1B73kzcE1e')
        return
    
    group_id = ev.group_id
    
    try:
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = await get_video_info(bvid)
        if not video_info:
            await bot.send(ev, 'è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åå†è¯•')
            return
        
        up_name = video_info['owner']['name']
        
        # æ£€æŸ¥æ˜¯å¦å·²å…³æ³¨
        if watch_storage.find_up_by_name(up_name):
            await bot.send(ev, f'ã€{up_name}ã€‘å·²åœ¨ç›‘æ§åˆ—è¡¨ä¸­')
            return
        
        # æ·»åŠ åˆ°ç›‘æ§
        watch_storage.add_watch(
            group_id=group_id,
            up_name=up_name,
            last_vid=bvid
        )
        
        await bot.send(ev, f'âœ… å·²é€šè¿‡è§†é¢‘å…³æ³¨UPä¸»ã€{up_name}ã€‘\n'
                         f'è§†é¢‘æ ‡é¢˜: {video_info["title"]}\n'
                         'å°†ç›‘æ§åç»­æ›´æ–°')
        
    except Exception as e:
        await bot.send(ev, f'é€šè¿‡è§†é¢‘å…³æ³¨å¤±è´¥: {str(e)}')

@sv.on_prefix('å–å…³up')
async def unwatch_bilibili_up(bot, ev: CQEvent):
    up_name = ev.message.extract_plain_text().strip()
    group_id = ev.group_id
    
    # æŸ¥æ‰¾å‡†ç¡®çš„UPä¸»åç§°
    found = watch_storage.find_up_by_name(up_name)
    if not found:
        await bot.send(ev, f'æœªæ‰¾åˆ°ã€{up_name}ã€‘çš„ç›‘æ§è®°å½•')
        return
    
    group_id_str, exact_name = found
    if watch_storage.remove_watch(int(group_id_str), exact_name):
        await bot.send(ev, f'âœ… å·²å–æ¶ˆå¯¹ã€{exact_name}ã€‘çš„ç›‘æ§')
    else:
        await bot.send(ev, 'å–å…³å¤±è´¥ï¼Œè¯·ç¨åå†è¯•')

@sv.on_fullmatch('æŸ¥çœ‹å…³æ³¨')
async def list_watched_ups(bot, ev: CQEvent):
    group_id = ev.group_id
    watches = watch_storage.get_group_watches(group_id)
    
    if not watches:
        await bot.send(ev, 'å½“å‰æ²¡æœ‰ç›‘æ§ä»»ä½•UPä¸»')
        return
    
    up_list = ["ğŸ“¢ å½“å‰ç›‘æ§çš„UPä¸»åˆ—è¡¨:", "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"]
    for up_name, info in watches.items():
        last_check = datetime.fromisoformat(info['last_check']).strftime('%m-%d %H:%M')
        up_list.append(f"ğŸ‘¤ {up_name} | æœ€åæ£€æŸ¥: {last_check}")
        up_list.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    await bot.send(ev, "\n".join(up_list))

@sv.scheduled_job('interval', minutes=UP_WATCH_INTERVAL)
async def check_up_updates():
    sv.logger.info("å¼€å§‹æ‰§è¡ŒUPä¸»ç›‘æ§æ£€æŸ¥...")
    all_watches = watch_storage.get_all_watches()
    if not all_watches:
        sv.logger.info("å½“å‰æ²¡æœ‰ç›‘æ§ä»»ä½•UPä¸»")
        return
    
    bot = sv.bot
    update_count = 0
    
    for group_id_str, up_dict in all_watches.items():
        group_id = int(group_id_str)
        for up_name, info in up_dict.items():
            try:
                last_vid = info.get('last_vid')
                sv.logger.info(f"æ£€æŸ¥UPä¸»ã€{up_name}ã€‘æ›´æ–°ï¼Œä¸Šæ¬¡è®°å½•è§†é¢‘: {last_vid or 'æ— '}")

                # 1. ä½¿ç”¨æŸ¥è§†é¢‘åŠŸèƒ½æœç´¢UPä¸»åç§°
                search_results = await get_bilibili_search(up_name, "video")
                if not search_results:
                    sv.logger.warning(f"æœªæ‰¾åˆ°ã€{up_name}ã€‘çš„è§†é¢‘")
                    continue
                
                # 2. ä¸¥æ ¼ç­›é€‰UPä¸»åç§°å®Œå…¨åŒ¹é…çš„è§†é¢‘ï¼ˆæœ€å¤š5ä¸ªï¼‰
                matched_videos = [
                    video for video in search_results[:5]  # åªæ£€æŸ¥å‰5ä¸ªç»“æœ
                    if normalize_name(video['author']) == normalize_name(up_name)
                ]
                
                if not matched_videos:
                    sv.logger.info(f"æœªæ‰¾åˆ°UPä¸»åç§°å®Œå…¨åŒ¹é…ã€{up_name}ã€‘çš„è§†é¢‘")
                    continue
                
                # 3. ä»åŒ¹é…ç»“æœä¸­è·å–æœ€æ–°è§†é¢‘
                latest_video = max(matched_videos, key=lambda x: x['pubdate'])
                current_bvid = latest_video['bvid']
                video_pub_time = datetime.fromtimestamp(latest_video['pubdate'])
                
                # 4. éªŒè¯æ˜¯å¦ä¸ºæ–°è§†é¢‘
                if not last_vid:
                    is_new = True
                    reason = "é¦–æ¬¡ç›‘æ§è¯¥UPä¸»"
                else:
                    is_new, reason = await verify_new_video(last_vid, current_bvid, video_pub_time)
                
                sv.logger.info(f"æ›´æ–°åˆ¤æ–­: {reason}")
                
                # 5. å¦‚æœæ˜¯æ–°è§†é¢‘åˆ™æ¨é€
                if is_new:
                    await send_new_video_notice(bot, group_id, up_name, latest_video)
                    watch_storage.update_last_video(group_id, up_name, current_bvid)
                    update_count += 1
                
            except Exception as e:
                sv.logger.error(f'ç›‘æ§UPä¸»ã€{up_name}ã€‘å¤±è´¥: {str(e)}')
                continue
    
    sv.logger.info(f"ç›‘æ§æ£€æŸ¥å®Œæˆï¼Œå…±æ£€æŸ¥ {len(all_watches)} ä¸ªUPä¸»ï¼Œå‘ç° {update_count} ä¸ªæ›´æ–°")

async def verify_new_video(last_vid: str, current_bvid: str, video_pub_time: datetime) -> Tuple[bool, str]:
    """éªŒè¯æ˜¯å¦ä¸ºæ–°è§†é¢‘"""
    # 1. æ£€æŸ¥BVå·æ˜¯å¦ç›¸åŒ
    if current_bvid == last_vid:
        return False, "BVå·ç›¸åŒï¼Œè§†é¢‘æœªæ›´æ–°"
    
    # 2. è·å–ä¸Šæ¬¡è§†é¢‘ä¿¡æ¯
    last_video_info = await get_video_info(last_vid)
    if not last_video_info:
        return False, "æ— æ³•è·å–ä¸Šæ¬¡è§†é¢‘ä¿¡æ¯ï¼Œä¿å®ˆå¤„ç†ä¸æ¨é€"
    
    last_pub_time = datetime.fromtimestamp(last_video_info['pubdate'])
    
    # 3. æ¯”è¾ƒå‘å¸ƒæ—¶é—´ï¼ˆå¢åŠ 2åˆ†é’Ÿç¼“å†²ï¼‰
    if video_pub_time > (last_pub_time + timedelta(minutes=2)):
        return True, f"æ–°è§†é¢‘å‘å¸ƒæ—¶é—´({video_pub_time}) > ä¸Šæ¬¡è§†é¢‘å‘å¸ƒæ—¶é—´({last_pub_time})"
    else:
        return False, f"å‘å¸ƒæ—¶é—´æœªè¶…è¿‡é˜ˆå€¼: {video_pub_time} â‰¤ {last_pub_time}+2åˆ†é’Ÿ"

async def send_new_video_notice(bot, group_id: int, up_name: str, video_info: dict):
    """å‘é€æ–°è§†é¢‘é€šçŸ¥"""
    pub_time = datetime.fromtimestamp(video_info['pubdate']).strftime("%Y-%m-%d %H:%M")
    pic_url = f"https:{video_info['pic']}" if not video_info['pic'].startswith(('http://', 'https://')) else video_info['pic']
    proxied_url = f'https://images.weserv.nl/?url={quote(pic_url.replace("https://", "").replace("http://", ""), safe="")}'
    
    msg = [
        f"ğŸ“¢ UPä¸»ã€{up_name}ã€‘å‘å¸ƒäº†æ–°è§†é¢‘ï¼",
        f"ğŸ“º æ ‡é¢˜: {video_info['title']}",
        f"[CQ:image,file={proxied_url}]",
        f"â° å‘å¸ƒæ—¶é—´: {pub_time}",
        f"ğŸ”— è§†é¢‘é“¾æ¥: https://b23.tv/{video_info['bvid']}"
    ]
    
    await bot.send_group_msg(group_id=group_id, message="\n".join(msg))
    sv.logger.info(f"å·²å‘é€æ–°è§†é¢‘é€šçŸ¥: {up_name} - {video_info['title']}")
        
@sv.on_prefix('æŸ¥è§†é¢‘')
async def search_bilibili_video(bot, ev: CQEvent):
    keyword = ev.message.extract_plain_text().strip()
    if not keyword:
        await bot.send(ev, 'è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼šæŸ¥è§†é¢‘ åŸç¥')
        return
    
    try:
        msg_id = (await bot.send(ev, "ğŸ” æœç´¢ä¸­..."))['message_id']
        results = await get_bilibili_search(keyword, "video")
        
        if not results:
            await bot.finish(ev, f'æœªæ‰¾åˆ°"{keyword}"ç›¸å…³è§†é¢‘')
            return

        reply = ["ğŸ“º æœç´¢ç»“æœï¼ˆæœ€å¤š5ä¸ªï¼‰ï¼š", "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"]
        for i, video in enumerate(results, 1):
            clean_title = re.sub(r'<[^>]+>', '', video['title'])
            pub_time = time.strftime("%Y-%m-%d", time.localtime(video['pubdate']))
            
            # å¤„ç†å›¾ç‰‡URL
            pic_url = video['pic']
            if not pic_url.startswith(('http://', 'https://')):
                pic_url = 'https:' + pic_url
            proxied_url = f'https://images.weserv.nl/?url={quote(pic_url.replace("https://", "").replace("http://", ""), safe="")}'
            
            reply.extend([
                f"{i}. {clean_title}",
                f"[CQ:image,file={proxied_url}]",
                f"   ğŸ“… {pub_time} | ğŸ‘¤ {video['author']}",
                f"   ğŸ”— https://b23.tv/{video['bvid']}",
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            ])
        
        await safe_send(bot, ev, "\n".join(reply))
    except Exception as e:
        await bot.send(ev, f'æœç´¢å¤±è´¥: {str(e)}')
