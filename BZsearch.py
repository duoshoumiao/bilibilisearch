import os
import json
import re
import time
import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from urllib.parse import quote

from hoshino import Service, priv
from hoshino.typing import CQEvent
import aiohttp

# ä¸»æœåŠ¡å®šä¹‰
sv = Service('bç«™è§†é¢‘æœç´¢', enable_on_default=True, help_='æœç´¢Bç«™è§†é¢‘\nä½¿ç”¨æ–¹æ³•ï¼š\n1. æŸ¥è§†é¢‘ [å…³é”®è¯] - æœç´¢Bç«™è§†é¢‘\n2. æŸ¥up [UPä¸»åç§°] - æœç´¢UPä¸»è§†é¢‘\n3. å…³æ³¨up [UPä¸»åç§°] - ç›‘æ§UPä¸»æ–°è§†é¢‘\n4. è§†é¢‘å…³æ³¨ [è§†é¢‘é“¾æ¥] - é€šè¿‡è§†é¢‘é“¾æ¥å…³æ³¨UPä¸»\n5. å–å…³up [UPä¸»åç§°] - å–æ¶ˆç›‘æ§\n6. æŸ¥çœ‹å…³æ³¨ - æŸ¥çœ‹å½“å‰ç›‘æ§åˆ—è¡¨')

# é…ç½®é¡¹
MAX_RESULTS = 5
UP_WATCH_INTERVAL = 1  # ç›‘æ§é—´éš”(åˆ†é’Ÿ)
CACHE_EXPIRE_MINUTES = 3
search_cache = {}

# JSONå­˜å‚¨æ–‡ä»¶è·¯å¾„
WATCH_JSON_PATH = Path(__file__).parent / 'data' / 'bili_watch.json'
os.makedirs(WATCH_JSON_PATH.parent, exist_ok=True)

class UpWatchStorage:
    def __init__(self):
        self._data = {}
        self.name_index = {}  # åç§°å°å†™ç´¢å¼•
        self._load_and_migrate()
        sv.logger.info("UPä¸»ç›‘æ§å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_and_migrate(self):
        """åŠ è½½å¹¶è‡ªåŠ¨è¿ç§»æ—§æ ¼å¼æ•°æ®"""
        try:
            if WATCH_JSON_PATH.exists():
                with open(WATCH_JSON_PATH, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                    sv.logger.info(f"ä»æ–‡ä»¶åŠ è½½ç›‘æ§æ•°æ®ï¼Œå…± {sum(len(v) for v in old_data.values())} æ¡è®°å½•")
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ—§æ ¼å¼(åŒ…å«æ•°å­—UIDé”®)
                    is_old_format = any(
                        any(k.isdigit() for k in ups.keys())
                        for ups in old_data.values()
                    )
                    
                    if is_old_format:
                        sv.logger.info("æ£€æµ‹åˆ°æ—§æ ¼å¼æ•°æ®ï¼Œå¼€å§‹è‡ªåŠ¨è¿ç§»...")
                        self._migrate_from_old_format(old_data)
                    else:
                        self._data = old_data
                        # é‡å»ºåç§°ç´¢å¼•
                        for group_id, ups in old_data.items():
                            for up_name in ups.keys():
                                self.name_index[up_name.lower()] = (group_id, up_name)
                        sv.logger.info("æ•°æ®åŠ è½½å®Œæˆï¼Œæ— éœ€è¿ç§»")
                        
        except Exception as e:
            sv.logger.error(f"åŠ è½½ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}")
            self._data = {}
    
    def _migrate_from_old_format(self, old_data):
        """ä»æ—§æ ¼å¼è¿ç§»æ•°æ®"""
        migrated_count = 0
        for group_id, ups in old_data.items():
            self._data[group_id] = {}
            for uid, info in ups.items():
                try:
                    up_name = info['up_name']
                    # ç¡®ä¿åç§°å”¯ä¸€æ€§
                    if up_name.lower() in self.name_index:
                        sv.logger.warning(f"å‘ç°é‡å¤UPä¸»åç§°: {up_name}ï¼Œæ·»åŠ éšæœºåç¼€")
                        up_name = f"{up_name}_{uid[-4:]}"
                    
                    self._data[group_id][up_name] = {
                        'last_check': info['last_check'],
                        'last_vid': info['last_vid']
                    }
                    self.name_index[up_name.lower()] = (group_id, up_name)
                    migrated_count += 1
                except Exception as e:
                    sv.logger.error(f"è¿ç§»UPä¸» {uid} å¤±è´¥: {str(e)}")
        
        sv.logger.info(f"æ•°æ®è¿ç§»å®Œæˆï¼Œå…±è¿ç§» {migrated_count} æ¡è®°å½•")
        self.save()  # ç«‹å³ä¿å­˜æ–°æ ¼å¼
    
    def save(self):
        try:
            with open(WATCH_JSON_PATH, 'w', encoding='utf-8') as f:
                json.dump(self._data, f, ensure_ascii=False, indent=2)
            sv.logger.info("ç›‘æ§æ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            sv.logger.error(f"ä¿å­˜ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}")
    
    def add_watch(self, group_id: int, up_name: str, last_vid: str = None):
        group_id = str(group_id)
        if group_id not in self._data:
            self._data[group_id] = {}
        
        # æ ‡å‡†åŒ–åç§°å­˜å‚¨
        self._data[group_id][up_name] = {
            'last_check': datetime.now().isoformat(),
            'last_vid': last_vid
        }
        self.name_index[up_name.lower()] = (group_id, up_name)
        self.save()
        sv.logger.info(f"å·²æ·»åŠ ç›‘æ§: ç¾¤{group_id} -> UPä¸»{up_name}")
    
    def remove_watch(self, group_id: int, up_name: str) -> bool:
        group_id = str(group_id)
        if group_id in self._data and up_name in self._data[group_id]:
            del self._data[group_id][up_name]
            if up_name.lower() in self.name_index:
                del self.name_index[up_name.lower()]
            if not self._data[group_id]:
                del self._data[group_id]
            self.save()
            sv.logger.info(f"å·²ç§»é™¤ç›‘æ§: ç¾¤{group_id} -> UPä¸»{up_name}")
            return True
        sv.logger.warning(f"ç§»é™¤ç›‘æ§å¤±è´¥: ç¾¤{group_id} æœªç›‘æ§ UPä¸»{up_name}")
        return False
    
    def get_group_watches(self, group_id: int) -> Dict[str, Dict[str, Any]]:
        group_id = str(group_id)
        return self._data.get(group_id, {})
    
    def get_all_watches(self) -> Dict[str, Any]:
        return self._data
    
    def update_last_video(self, group_id: int, up_name: str, last_vid: str):
        group_id = str(group_id)
        if group_id in self._data and up_name in self._data[group_id]:
            self._data[group_id][up_name].update({
                'last_vid': last_vid,
                'last_check': datetime.now().isoformat()
            })
            self.save()
            sv.logger.info(f"æ›´æ–°è§†é¢‘è®°å½•: ç¾¤{group_id} -> UPä¸»{up_name} -> BV{last_vid}")
    
    def find_up_by_name(self, name: str) -> Optional[Tuple[str, str]]:
        """é€šè¿‡åç§°æŸ¥æ‰¾(ä¸åŒºåˆ†å¤§å°å†™)"""
        return self.name_index.get(normalize_name(name))

# å…¨å±€å­˜å‚¨å®ä¾‹
watch_storage = UpWatchStorage()

def normalize_name(name: str) -> str:
    """æ ‡å‡†åŒ–åç§°(å»å‰åç©ºæ ¼/å°å†™)"""
    return name.strip().lower()

async def get_up_info_by_name(name: str) -> Optional[Dict]:
    """é€šè¿‡åç§°è·å–UPä¸»ä¿¡æ¯(æœç´¢API)"""
    url = "https://api.bilibili.com/x/web-interface/search/type"
    params = {
        'search_type': 'bili_user',
        'keyword': name,
        'page_size': 1
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': 'https://www.bilibili.com/',
        'Origin': 'https://www.bilibili.com',
        'Accept': 'application/json, text/plain, */*',
        'Cookie': 'buvid3=XXXXXX;'  # æ·»åŠ å¿…è¦çš„cookie
    }
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, params=params, headers=headers, timeout=10) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    if data.get('code') == 0 and data['data'].get('result'):
                        for user in data['data']['result']:
                            if normalize_name(user['uname']) == normalize_name(name):
                                return {
                                    'mid': str(user['mid']),
                                    'name': user['uname']
                                }
        except Exception as e:
            sv.logger.error(f"è·å–UPä¸»ä¿¡æ¯å¤±è´¥: {str(e)}")
    return None

async def get_up_videos(mid: str) -> list:
    """è·å–UPä¸»ç©ºé—´è§†é¢‘åˆ—è¡¨"""
    url = "https://api.bilibili.com/x/space/wbi/arc/search"
    params = {
        'mid': mid,
        'ps': MAX_RESULTS,
        'order': 'pubdate'
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': f'https://space.bilibili.com/{mid}/',
        'Origin': 'https://space.bilibili.com',
        'Cookie': 'buvid3=XXXXXX;'  # æ·»åŠ å¿…è¦çš„cookie
    }
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, params=params, headers=headers, timeout=10) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    if data.get('code') == 0:
                        return [{
                            'bvid': v['bvid'],
                            'title': v['title'],
                            'author': data['data']['list']['name'],
                            'pubdate': v['created'],
                            'pic': v['pic']
                        } for v in data['data']['list']['vlist']]
        except Exception as e:
            sv.logger.error(f"è·å–UPä¸»è§†é¢‘å¤±è´¥: {str(e)}")
    return []

async def get_bilibili_search(keyword: str, search_type: str = "video") -> list:
    cache_key = f"{search_type}:{normalize_name(keyword)}"
    if cache_key in search_cache:
        cached_data, timestamp = search_cache[cache_key]
        if datetime.now() - timestamp < timedelta(minutes=CACHE_EXPIRE_MINUTES):
            return cached_data[:MAX_RESULTS]

    params = {
        'search_type': 'video',
        'keyword': keyword,
        'order': 'pubdate' if search_type == "up" else 'totalrank',
        'ps': MAX_RESULTS,
        'platform': 'web'
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': 'https://www.bilibili.com/',
        'Origin': 'https://www.bilibili.com',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'no-cache',
        'Cookie': 'buvid3=XXXXXX;'  # æ·»åŠ å¿…è¦çš„cookie
    }

    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(
                'https://api.bilibili.com/x/web-interface/search/type',
                params=params,
                headers=headers,
                timeout=10
            ) as resp:
                if resp.status != 200:
                    sv.logger.error(f"æœç´¢è¯·æ±‚å¤±è´¥: HTTP {resp.status}")
                    return []
                data = await resp.json()
                if data.get('code') == 0:
                    results = data['data'].get('result', [])[:MAX_RESULTS]
                    if search_type == "up":
                        # ç¡®ä¿ç»“æœä¸­åŒ…å«ä½œè€…åå­—æ®µ
                        results = [v for v in results if 'author' in v]
                    search_cache[cache_key] = (results, datetime.now())
                    return results
        except Exception as e:
            sv.logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        return []

async def get_video_info(bvid: str) -> Optional[Dict]:
    """è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': f'https://www.bilibili.com/video/{bvid}'
    }
    url = f'https://api.bilibili.com/x/web-interface/view?bvid={bvid}'
    
    async with aiohttp.ClientSession() as session:
        try:
            sv.logger.info(f"è·å–è§†é¢‘ä¿¡æ¯: BV{bvid}")
            async with session.get(url, headers=headers, timeout=10) as resp:
                if resp.status != 200:
                    sv.logger.error(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: HTTP {resp.status}")
                    return None
                data = await resp.json()
                if data.get('code') == 0:
                    sv.logger.info(f"æˆåŠŸè·å–è§†é¢‘ä¿¡æ¯: {data['data']['title']}")
                    return data['data']
                else:
                    sv.logger.error(f"è§†é¢‘APIè¿”å›é”™è¯¯: {data.get('message')}")
        except Exception as e:
            sv.logger.error(f"è·å–è§†é¢‘ä¿¡æ¯å¼‚å¸¸: {str(e)}")
    return None

async def safe_send(bot, ev, message):
    """å®‰å…¨å‘é€æ¶ˆæ¯"""
    try:
        if not message:
            return
            
        if isinstance(message, list):
            message = '\n'.join(message)
            
        await bot.send(ev, message)
    except Exception as e:
        sv.logger.error(f'å‘é€æ¶ˆæ¯å¤±è´¥: {str(e)}')

@sv.on_prefix('å…³æ³¨up')
async def watch_bilibili_up(bot, ev: CQEvent):
    up_name = ev.message.extract_plain_text().strip()
    if not up_name:
        await bot.send(ev, 'è¯·è¾“å…¥UPä¸»åç§°ï¼Œä¾‹å¦‚ï¼šå…³æ³¨up è€ç•ªèŒ„')
        return
    
    group_id = ev.group_id
    
    # æ£€æŸ¥æ˜¯å¦å·²å…³æ³¨
    if watch_storage.find_up_by_name(up_name):
        await bot.send(ev, f'ã€{up_name}ã€‘å·²åœ¨ç›‘æ§åˆ—è¡¨ä¸­')
        return
    
    # è·å–UPä¸»ä¿¡æ¯
    up_info = await get_up_info_by_name(up_name)
    if not up_info:
        await bot.send(ev, f'æœªæ‰¾åˆ°UPä¸»ã€{up_name}ã€‘ï¼Œè¯·ç¡®è®¤åç§°æ˜¯å¦æ­£ç¡®')
        return
    
    # è·å–UPä¸»æœ€æ–°è§†é¢‘
    videos = await get_up_videos(up_info['mid'])
    if not videos:
        await bot.send(ev, f'UPä¸»ã€{up_name}ã€‘æ²¡æœ‰å‘å¸ƒè¿‡è§†é¢‘')
        return
    
    latest_video = videos[0]
    
    # æ·»åŠ åˆ°ç›‘æ§
    watch_storage.add_watch(
        group_id=group_id,
        up_name=up_info['name'],
        last_vid=latest_video['bvid']
    )
    
    await bot.send(ev, f'âœ… å·²å…³æ³¨UPä¸»ã€{up_info["name"]}ã€‘\n'
                      f'æœ€æ–°è§†é¢‘: {latest_video["title"]}\n'
                      'å°†ç›‘æ§åç»­æ›´æ–°')

@sv.on_prefix('è§†é¢‘å…³æ³¨')
async def watch_by_video(bot, ev: CQEvent):
    """é€šè¿‡è§†é¢‘é“¾æ¥å…³æ³¨UPä¸»"""
    video_url = ev.message.extract_plain_text().strip()
    if not video_url:
        await bot.send(ev, 'è¯·è¾“å…¥è§†é¢‘é“¾æ¥ï¼Œä¾‹å¦‚ï¼šè§†é¢‘å…³æ³¨ https://www.bilibili.com/video/BV1xxx')
        return
    
    # æå–BVå·
    bvid = None
    if 'bilibili.com/video/' in video_url:
        match = re.search(r'bilibili\.com/video/(BV[0-9A-Za-z]+)', video_url)
        if match:
            bvid = match.group(1)
    
    if not bvid:
        await bot.send(ev, 'æ— æ³•ä»é“¾æ¥ä¸­è¯†åˆ«è§†é¢‘BVå·ï¼Œè¯·ç¡®è®¤é“¾æ¥æ ¼å¼æ­£ç¡®')
        return
    
    group_id = ev.group_id
    
    try:
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = await get_video_info(bvid)
        if not video_info:
            await bot.send(ev, 'è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åå†è¯•')
            return
        
        up_name = video_info['owner']['name']
        
        # æ£€æŸ¥æ˜¯å¦å·²å…³æ³¨
        if watch_storage.find_up_by_name(up_name):
            await bot.send(ev, f'ã€{up_name}ã€‘å·²åœ¨ç›‘æ§åˆ—è¡¨ä¸­')
            return
        
        # æ·»åŠ åˆ°ç›‘æ§
        watch_storage.add_watch(
            group_id=group_id,
            up_name=up_name,
            last_vid=bvid
        )
        
        await bot.send(ev, f'âœ… å·²é€šè¿‡è§†é¢‘å…³æ³¨UPä¸»ã€{up_name}ã€‘\n'
                         f'è§†é¢‘æ ‡é¢˜: {video_info["title"]}\n'
                         'å°†ç›‘æ§åç»­æ›´æ–°')
        
    except Exception as e:
        await bot.send(ev, f'é€šè¿‡è§†é¢‘å…³æ³¨å¤±è´¥: {str(e)}')

@sv.on_prefix('å–å…³up')
async def unwatch_bilibili_up(bot, ev: CQEvent):
    up_name = ev.message.extract_plain_text().strip()
    group_id = ev.group_id
    
    # æŸ¥æ‰¾å‡†ç¡®çš„UPä¸»åç§°
    found = watch_storage.find_up_by_name(up_name)
    if not found:
        await bot.send(ev, f'æœªæ‰¾åˆ°ã€{up_name}ã€‘çš„ç›‘æ§è®°å½•')
        return
    
    group_id_str, exact_name = found
    if watch_storage.remove_watch(int(group_id_str), exact_name):
        await bot.send(ev, f'âœ… å·²å–æ¶ˆå¯¹ã€{exact_name}ã€‘çš„ç›‘æ§')
    else:
        await bot.send(ev, 'å–å…³å¤±è´¥ï¼Œè¯·ç¨åå†è¯•')

@sv.on_fullmatch('æŸ¥çœ‹å…³æ³¨')
async def list_watched_ups(bot, ev: CQEvent):
    group_id = ev.group_id
    watches = watch_storage.get_group_watches(group_id)
    
    if not watches:
        await bot.send(ev, 'å½“å‰æ²¡æœ‰ç›‘æ§ä»»ä½•UPä¸»')
        return
    
    up_list = ["ğŸ“¢ å½“å‰ç›‘æ§çš„UPä¸»åˆ—è¡¨:", "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"]
    for up_name, info in watches.items():
        last_check = datetime.fromisoformat(info['last_check']).strftime('%m-%d %H:%M')
        up_list.append(f"ğŸ‘¤ {up_name} | æœ€åæ£€æŸ¥: {last_check}")
        up_list.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    await bot.send(ev, "\n".join(up_list))

@sv.scheduled_job('interval', minutes=UP_WATCH_INTERVAL)
async def check_up_updates():
    sv.logger.info("å¼€å§‹æ‰§è¡ŒUPä¸»ç›‘æ§æ£€æŸ¥...")
    all_watches = watch_storage.get_all_watches()
    if not all_watches:
        sv.logger.info("å½“å‰æ²¡æœ‰ç›‘æ§ä»»ä½•UPä¸»")
        return
    
    bot = sv.bot
    update_count = 0
    
    for group_id_str, up_dict in all_watches.items():
        group_id = int(group_id_str)
        for up_name, info in up_dict.items():
            try:
                last_vid = info.get('last_vid')
                last_check_time = datetime.fromisoformat(info['last_check'])
                sv.logger.info(f"æ£€æŸ¥UPä¸»ã€{up_name}ã€‘æ›´æ–°ï¼Œä¸Šæ¬¡è®°å½•è§†é¢‘: {last_vid}")
                
                # è·å–æœ€æ–°è§†é¢‘
                results = await get_bilibili_search(up_name, "up")
                if not results:
                    sv.logger.warning(f"æœç´¢APIæœªæ‰¾åˆ°ã€{up_name}ã€‘çš„è§†é¢‘ï¼Œå°è¯•ç©ºé—´API")
                    up_info = await get_up_info_by_name(up_name)
                    if not up_info:
                        continue
                    videos = await get_up_videos(up_info['mid'])
                    if not videos:
                        continue
                    latest_video = videos[0]
                    video_pub_time = datetime.fromtimestamp(latest_video['pubdate'])
                else:
                    # ç¡®ä¿æ‰¾åˆ°çš„è§†é¢‘ç¡®å®æ˜¯è¯¥UPä¸»çš„
                    latest_video = None
                    for video in results:
                        if normalize_name(video['author']) == normalize_name(up_name):
                            latest_video = video
                            break
                    if not latest_video:
                        continue
                    video_pub_time = datetime.fromtimestamp(latest_video['pubdate'])
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°è§†é¢‘
                is_new = False
                if not last_vid:
                    is_new = True
                    reason = "é¦–æ¬¡ç›‘æ§"
                else:
                    last_video_info = await get_video_info(last_vid)
                    if not last_video_info:
                        is_new = True
                        reason = "æ— æ³•è·å–ä¸Šæ¬¡è§†é¢‘ä¿¡æ¯"
                    else:
                        last_pub_time = datetime.fromtimestamp(last_video_info['pubdate'])
                        is_new = video_pub_time > last_pub_time
                        reason = f"æ–°å‘å¸ƒæ—¶é—´({video_pub_time}) > ä¸Šæ¬¡å‘å¸ƒæ—¶é—´({last_pub_time})" if is_new else "æ— æ–°å‘å¸ƒ"
                
                sv.logger.info(f"æ›´æ–°åˆ¤æ–­: {reason}")
                
                # æ— è®ºæ˜¯å¦æœ‰æ›´æ–°ï¼Œéƒ½æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´å’Œè§†é¢‘ID
                watch_storage.update_last_video(
                    group_id=group_id,
                    up_name=up_name,
                    last_vid=latest_video['bvid']  # æ€»æ˜¯ä½¿ç”¨æœ€æ–°æ‰¾åˆ°çš„è§†é¢‘ID
                )
                
                if is_new:
                    # å‡†å¤‡å¹¶å‘é€é€šçŸ¥
                    pub_time = video_pub_time.strftime("%Y-%m-%d %H:%M")
                    pic_url = latest_video['pic']
                    if not pic_url.startswith(('http://', 'https://')):
                        pic_url = 'https:' + pic_url
                    proxied_url = f'https://images.weserv.nl/?url={quote(pic_url.replace("https://", "").replace("http://", ""), safe="")}'
                    
                    msg = [
                        f"ğŸ“¢ UPä¸»ã€{up_name}ã€‘å‘å¸ƒäº†æ–°è§†é¢‘ï¼",
                        f"ğŸ“º æ ‡é¢˜: {latest_video['title']}",
                        f"[CQ:image,file={proxied_url}]",
                        f"â° å‘å¸ƒæ—¶é—´: {pub_time}",
                        f"ğŸ”— è§†é¢‘é“¾æ¥: https://b23.tv/{latest_video['bvid']}"
                    ]
                    
                    await bot.send_group_msg(group_id=group_id, message="\n".join(msg))
                    update_count += 1
                    sv.logger.info(f"å·²å‘é€æ–°è§†é¢‘é€šçŸ¥: {up_name} - {latest_video['title']}")
                
            except Exception as e:
                sv.logger.error(f'ç›‘æ§UPä¸»ã€{up_name}ã€‘å¤±è´¥: {str(e)}')
                continue
    
    sv.logger.info(f"ç›‘æ§æ£€æŸ¥å®Œæˆï¼Œå…±æ£€æŸ¥ {sum(len(v) for v in all_watches.values())} ä¸ªUPä¸»ï¼Œå‘ç° {update_count} ä¸ªæ›´æ–°")
    
@sv.on_prefix('æŸ¥è§†é¢‘')
async def search_bilibili_video(bot, ev: CQEvent):
    keyword = ev.message.extract_plain_text().strip()
    if not keyword:
        await bot.send(ev, 'è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼šæŸ¥è§†é¢‘ åŸç¥')
        return
    
    try:
        msg_id = (await bot.send(ev, "ğŸ” æœç´¢ä¸­..."))['message_id']
        results = await get_bilibili_search(keyword, "video")
        
        if not results:
            await bot.finish(ev, f'æœªæ‰¾åˆ°"{keyword}"ç›¸å…³è§†é¢‘')
            return

        reply = ["ğŸ“º æœç´¢ç»“æœï¼ˆæœ€å¤š5ä¸ªï¼‰ï¼š", "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"]
        for i, video in enumerate(results, 1):
            clean_title = re.sub(r'<[^>]+>', '', video['title'])
            pub_time = time.strftime("%Y-%m-%d", time.localtime(video['pubdate']))
            
            # å¤„ç†å›¾ç‰‡URL
            pic_url = video['pic']
            if not pic_url.startswith(('http://', 'https://')):
                pic_url = 'https:' + pic_url
            proxied_url = f'https://images.weserv.nl/?url={quote(pic_url.replace("https://", "").replace("http://", ""), safe="")}'
            
            reply.extend([
                f"{i}. {clean_title}",
                f"[CQ:image,file={proxied_url}]",
                f"   ğŸ“… {pub_time} | ğŸ‘¤ {video['author']}",
                f"   ğŸ”— https://b23.tv/{video['bvid']}",
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            ])
        
        await safe_send(bot, ev, "\n".join(reply))
    except Exception as e:
        await bot.send(ev, f'æœç´¢å¤±è´¥: {str(e)}')

@sv.on_prefix('æŸ¥up')
async def search_bilibili_up(bot, ev: CQEvent):
    up_name = ev.message.extract_plain_text().strip()
    if not up_name:
        await bot.send(ev, 'è¯·è¾“å…¥UPä¸»åç§°ï¼Œä¾‹å¦‚ï¼šæŸ¥up è€ç•ªèŒ„')
        return

    try:
        msg_id = (await bot.send(ev, f"ğŸ”ğŸ” æ­£åœ¨æœç´¢ã€{up_name}ã€‘çš„æœ€æ–°è§†é¢‘..."))['message_id']
        
        results = await get_bilibili_search(up_name, "up")
        if not results:
            await bot.finish(ev, f'æœªæ‰¾åˆ°UPä¸»ã€{up_name}ã€‘çš„è§†é¢‘')
            return

        reply = [f"ğŸ‘¤ğŸ‘¤ {results[0]['author']} (UID:{results[0]['mid']}) çš„æœç´¢ç»“æœï¼ˆæœ€å¤š5ä¸ªï¼‰ï¼š", "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"]
        for i, video in enumerate(results, 1):
            pub_time = time.strftime("%Y-%m-%d", time.localtime(video['pubdate']))
            
            # å¤„ç†å›¾ç‰‡URL
            pic_url = video['pic']
            if not pic_url.startswith(('http://', 'https://')):
                pic_url = 'https:' + pic_url
            proxied_url = f'https://images.weserv.nl/?url={quote(pic_url.replace("https://", "").replace("http://", ""), safe="")}'
            
            reply.extend([
                f"{i}. {re.sub(r'<[^>]+>', '', video['title'])}",
                f"[CQ:image,file={proxied_url}]",  # å›¾ç‰‡æ”¾åœ¨æ ‡é¢˜ä¸‹æ–¹
                f"   ğŸ“…ğŸ“… {pub_time} | ğŸ‘€ğŸ‘€ {video.get('play', 0)}æ’­æ”¾",
                f"   ğŸ”—ğŸ”— https://b23.tv/{video['bvid']}",
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            ])
        await safe_send(bot, ev, "\n".join(reply))

    except Exception as e:
        await bot.send(ev, f'æœç´¢å¤±è´¥: {str(e)}')



@sv.scheduled_job('interval', minutes=3)
async def clear_cache():
    global search_cache
    expired_keys = [k for k, (_, t) in search_cache.items() 
                   if datetime.now() - t > timedelta(minutes=CACHE_EXPIRE_MINUTES)]
    for k in expired_keys:
        del search_cache[k]
    sv.logger.info(f"æ¸…ç†ç¼“å­˜ï¼Œç§»é™¤ {len(expired_keys)} æ¡è¿‡æœŸè®°å½•")
