import os
import json
import re
import time
import asyncio
import difflib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from urllib.parse import quote
import random
import string

from hoshino import Service, priv
from hoshino.typing import CQEvent
import aiohttp

# ä¸»æœåŠ¡å®šä¹‰
sv = Service('bç«™è§†é¢‘æœç´¢', enable_on_default=True, help_='æœç´¢Bç«™è§†é¢‘\nä½¿ç”¨æ–¹æ³•ï¼š\n1. æŸ¥è§†é¢‘ [å…³é”®è¯] - æœç´¢Bç«™è§†é¢‘\n2. æŸ¥up [UPä¸»åç§°] - æœç´¢UPä¸»è§†é¢‘\n3. å…³æ³¨up [UPä¸»åç§°] - ç›‘æ§UPä¸»æ–°è§†é¢‘\n4. è§†é¢‘å…³æ³¨ [è§†é¢‘é“¾æ¥] - é€šè¿‡è§†é¢‘é“¾æ¥å…³æ³¨UPä¸»\n5. å–å…³up [UPä¸»åç§°] - å–æ¶ˆç›‘æ§\n6. æŸ¥çœ‹å…³æ³¨ - æŸ¥çœ‹å½“å‰ç›‘æ§åˆ—è¡¨')

# é…ç½®é¡¹
MAX_RESULTS = 5
UP_WATCH_INTERVAL = 10
CACHE_EXPIRE_MINUTES = 3
search_cache = {}

# JSONå­˜å‚¨æ–‡ä»¶è·¯å¾„
WATCH_JSON_PATH = Path(__file__).parent / 'data' / 'bili_watch.json'
os.makedirs(WATCH_JSON_PATH.parent, exist_ok=True)

class UpWatchStorage:
    def __init__(self):
        self._data = {}
        self.name_index = {}  # åç§°å°å†™ç´¢å¼•
        self._load_and_migrate()
    
    def _load_and_migrate(self):
        """åŠ è½½å¹¶è‡ªåŠ¨è¿ç§»æ—§æ ¼å¼æ•°æ®"""
        try:
            if WATCH_JSON_PATH.exists():
                with open(WATCH_JSON_PATH, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ—§æ ¼å¼(åŒ…å«æ•°å­—UIDé”®)
                    is_old_format = any(
                        any(k.isdigit() for k in ups.keys())
                        for ups in old_data.values()
                    )
                    
                    if is_old_format:
                        sv.logger.info("æ£€æµ‹åˆ°æ—§æ ¼å¼æ•°æ®ï¼Œå¼€å§‹è‡ªåŠ¨è¿ç§»...")
                        self._migrate_from_old_format(old_data)
                    else:
                        self._data = old_data
                        # é‡å»ºåç§°ç´¢å¼•
                        for group_id, ups in old_data.items():
                            for up_name in ups.keys():
                                self.name_index[up_name.lower()] = (group_id, up_name)
                        
        except Exception as e:
            sv.logger.error(f"åŠ è½½ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}")
            self._data = {}
    
    def _migrate_from_old_format(self, old_data):
        """ä»æ—§æ ¼å¼è¿ç§»æ•°æ®"""
        migrated_count = 0
        for group_id, ups in old_data.items():
            self._data[group_id] = {}
            for uid, info in ups.items():
                try:
                    up_name = info['up_name']
                    # ç¡®ä¿åç§°å”¯ä¸€æ€§
                    if up_name.lower() in self.name_index:
                        sv.logger.warning(f"å‘ç°é‡å¤UPä¸»åç§°: {up_name}ï¼Œæ·»åŠ éšæœºåç¼€")
                        up_name = f"{up_name}_{uid[-4:]}"
                    
                    self._data[group_id][up_name] = {
                        'last_check': info['last_check'],
                        'last_vid': info['last_vid']
                    }
                    self.name_index[up_name.lower()] = (group_id, up_name)
                    migrated_count += 1
                except Exception as e:
                    sv.logger.error(f"è¿ç§»UPä¸» {uid} å¤±è´¥: {str(e)}")
        
        sv.logger.info(f"æ•°æ®è¿ç§»å®Œæˆï¼Œå…±è¿ç§» {migrated_count} æ¡è®°å½•")
        self.save()  # ç«‹å³ä¿å­˜æ–°æ ¼å¼
    
    def save(self):
        try:
            with open(WATCH_JSON_PATH, 'w', encoding='utf-8') as f:
                json.dump(self._data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            sv.logger.error(f"ä¿å­˜ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}")
    
    def add_watch(self, group_id: int, up_name: str, last_vid: str = None):
        group_id = str(group_id)
        if group_id not in self._data:
            self._data[group_id] = {}
        
        # æ ‡å‡†åŒ–åç§°å­˜å‚¨
        self._data[group_id][up_name] = {
            'last_check': datetime.now().isoformat(),
            'last_vid': last_vid
        }
        self.name_index[up_name.lower()] = (group_id, up_name)
        self.save()
    
    def remove_watch(self, group_id: int, up_name: str) -> bool:
        group_id = str(group_id)
        if group_id in self._data and up_name in self._data[group_id]:
            del self._data[group_id][up_name]
            if up_name.lower() in self.name_index:
                del self.name_index[up_name.lower()]
            if not self._data[group_id]:
                del self._data[group_id]
            self.save()
            return True
        return False
    
    def get_group_watches(self, group_id: int) -> Dict[str, Dict[str, Any]]:
        group_id = str(group_id)
        return self._data.get(group_id, {})
    
    def get_all_watches(self) -> Dict[str, Any]:
        return self._data
    
    def update_last_video(self, group_id: int, up_name: str, last_vid: str):
        group_id = str(group_id)
        if group_id in self._data and up_name in self._data[group_id]:
            self._data[group_id][up_name].update({
                'last_vid': last_vid,
                'last_check': datetime.now().isoformat()
            })
            self.save()
    
    def find_up_by_name(self, name: str) -> Optional[Tuple[str, str]]:
        """é€šè¿‡åç§°æŸ¥æ‰¾(ä¸åŒºåˆ†å¤§å°å†™)"""
        return self.name_index.get(normalize_name(name))

# å…¨å±€å­˜å‚¨å®ä¾‹
watch_storage = UpWatchStorage()

def normalize_name(name: str) -> str:
    """æ ‡å‡†åŒ–åç§°(å»å‰åç©ºæ ¼/å°å†™)"""
    return name.strip().lower()

async def get_bilibili_search(keyword: str, search_type: str = "video") -> list:
    cache_key = f"{search_type}:{normalize_name(keyword)}"
    if cache_key in search_cache:
        cached_data, timestamp = search_cache[cache_key]
        if datetime.now() - timestamp < timedelta(minutes=CACHE_EXPIRE_MINUTES):
            return cached_data[:MAX_RESULTS]

    params = {
        'search_type': 'video',
        'keyword': keyword,
        'order': 'pubdate' if search_type == "up" else 'totalrank',
        'ps': MAX_RESULTS,
        'platform': 'web'
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Cookie': 'buvid3=XXXXXX'
    }

    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(
                'https://api.bilibili.com/x/web-interface/search/type',
                params=params,
                headers=headers,
                timeout=10
            ) as resp:
                if resp.status != 200:
                    return []
                data = await resp.json()
                if data.get('code') == 0:
                    results = data['data'].get('result', [])[:MAX_RESULTS]
                    if search_type == "up":
                        # ç¡®ä¿ç»“æœä¸­åŒ…å«ä½œè€…åå­—æ®µ
                        results = [v for v in results if 'author' in v]
                    search_cache[cache_key] = (results, datetime.now())
                    return results
        except Exception as e:
            sv.logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        return []

async def get_video_info(bvid: str) -> Optional[Dict]:
    """è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
    }
    url = f'https://api.bilibili.com/x/web-interface/view?bvid={bvid}'
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers=headers, timeout=10) as resp:
                if resp.status != 200:
                    return None
                data = await resp.json()
                if data.get('code') == 0:
                    return data['data']
        except Exception:
            return None
    return None

async def safe_send(bot, ev, message):
    """å®‰å…¨å‘é€æ¶ˆæ¯"""
    try:
        if not message or message.strip() == '':
            return
            
        if isinstance(message, list):
            message = '\n'.join(message)
            
        await bot.send(ev, message)
    except Exception as e:
        sv.logger.error(f'å‘é€æ¶ˆæ¯å¤±è´¥: {str(e)}')



@sv.on_prefix('å…³æ³¨up')
async def watch_bilibili_up(bot, ev: CQEvent):
    up_name = ev.message.extract_plain_text().strip()
    if not up_name:
        await bot.send(ev, 'è¯·è¾“å…¥UPä¸»åç§°ï¼Œä¾‹å¦‚ï¼šå…³æ³¨up è€ç•ªèŒ„')
        return
    
    group_id = ev.group_id
    
    # æ£€æŸ¥æ˜¯å¦å·²å…³æ³¨
    if watch_storage.find_up_by_name(up_name):
        await bot.send(ev, f'ã€{up_name}ã€‘å·²åœ¨ç›‘æ§åˆ—è¡¨ä¸­')
        return
    
    # æœç´¢UPä¸»æœ€æ–°è§†é¢‘
    results = await get_bilibili_search(up_name, "up")
    if not results:
        await bot.send(ev, f'æœªæ‰¾åˆ°UPä¸»ã€{up_name}ã€‘çš„è§†é¢‘')
        return
    
    # ç²¾ç¡®åŒ¹é…åç§°
    exact_match = next(
        (v for v in results 
         if normalize_name(v['author']) == normalize_name(up_name)),
        None
    )
    
    if not exact_match:
        await bot.send(ev, f'æœªæ‰¾åˆ°åç§°å®Œå…¨åŒ¹é…çš„UPä¸»ã€{up_name}ã€‘')
        return
    
    # æ·»åŠ åˆ°ç›‘æ§
    watch_storage.add_watch(
        group_id=group_id,
        up_name=exact_match['author'],  # ä½¿ç”¨APIè¿”å›çš„æ ‡å‡†åç§°
        last_vid=exact_match['bvid']
    )
    
    await bot.send(ev, f'âœ… å·²å…³æ³¨UPä¸»ã€{exact_match["author"]}ã€‘\n'
                      f'æœ€æ–°è§†é¢‘: {exact_match["title"]}\n'
                      'å°†ç›‘æ§åç»­æ›´æ–°')

@sv.on_prefix('è§†é¢‘å…³æ³¨')
async def watch_by_video(bot, ev: CQEvent):
    """é€šè¿‡è§†é¢‘é“¾æ¥å…³æ³¨UPä¸»"""
    video_url = ev.message.extract_plain_text().strip()
    if not video_url:
        await bot.send(ev, 'è¯·è¾“å…¥è§†é¢‘é“¾æ¥ï¼Œä¾‹å¦‚ï¼šè§†é¢‘å…³æ³¨ https://www.bilibili.com/video/BV1xxx')
        return
    
    # æå–BVå·
    bvid = None
    if 'bilibili.com/video/' in video_url:
        match = re.search(r'bilibili\.com/video/(BV[0-9A-Za-z]+)', video_url)
        if match:
            bvid = match.group(1)
    
    if not bvid:
        await bot.send(ev, 'æ— æ³•ä»é“¾æ¥ä¸­è¯†åˆ«è§†é¢‘BVå·ï¼Œè¯·ç¡®è®¤é“¾æ¥æ ¼å¼æ­£ç¡®')
        return
    
    group_id = ev.group_id
    
    try:
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = await get_video_info(bvid)
        if not video_info:
            await bot.send(ev, 'è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åå†è¯•')
            return
        
        up_name = video_info['owner']['name']
        
        # æ£€æŸ¥æ˜¯å¦å·²å…³æ³¨
        if watch_storage.find_up_by_name(up_name):
            await bot.send(ev, f'ã€{up_name}ã€‘å·²åœ¨ç›‘æ§åˆ—è¡¨ä¸­')
            return
        
        # æ·»åŠ åˆ°ç›‘æ§
        watch_storage.add_watch(
            group_id=group_id,
            up_name=up_name,
            last_vid=bvid
        )
        
        await bot.send(ev, f'âœ… å·²é€šè¿‡è§†é¢‘å…³æ³¨UPä¸»ã€{up_name}ã€‘\n'
                         f'è§†é¢‘æ ‡é¢˜: {video_info["title"]}\n'
                         'å°†ç›‘æ§åç»­æ›´æ–°')
        
    except Exception as e:
        await bot.send(ev, f'é€šè¿‡è§†é¢‘å…³æ³¨å¤±è´¥: {str(e)}')

@sv.on_prefix('å–å…³up')
async def unwatch_bilibili_up(bot, ev: CQEvent):
    up_name = ev.message.extract_plain_text().strip()
    group_id = ev.group_id
    
    # æŸ¥æ‰¾å‡†ç¡®çš„UPä¸»åç§°
    found = watch_storage.find_up_by_name(up_name)
    if not found:
        await bot.send(ev, f'æœªæ‰¾åˆ°ã€{up_name}ã€‘çš„ç›‘æ§è®°å½•')
        return
    
    group_id_str, exact_name = found
    if watch_storage.remove_watch(int(group_id_str), exact_name):
        await bot.send(ev, f'âœ… å·²å–æ¶ˆå¯¹ã€{exact_name}ã€‘çš„ç›‘æ§')
    else:
        await bot.send(ev, 'å–å…³å¤±è´¥ï¼Œè¯·ç¨åå†è¯•')

@sv.on_fullmatch('æŸ¥çœ‹å…³æ³¨')
async def list_watched_ups(bot, ev: CQEvent):
    group_id = ev.group_id
    watches = watch_storage.get_group_watches(group_id)
    
    if not watches:
        await bot.send(ev, 'å½“å‰æ²¡æœ‰ç›‘æ§ä»»ä½•UPä¸»')
        return
    
    up_list = ["ğŸ“¢ å½“å‰ç›‘æ§çš„UPä¸»åˆ—è¡¨:", "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"]
    for up_name, info in watches.items():
        last_check = datetime.fromisoformat(info['last_check']).strftime('%m-%d %H:%M')
        up_list.append(f"ğŸ‘¤ {up_name} | æœ€åæ£€æŸ¥: {last_check}")
        up_list.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    await bot.send(ev, "\n".join(up_list))

@sv.scheduled_job('interval', minutes=UP_WATCH_INTERVAL)
async def check_up_updates():
    all_watches = watch_storage.get_all_watches()
    if not all_watches:
        return
    
    bot = sv.bot
    
    for group_id_str, up_dict in all_watches.items():
        group_id = int(group_id_str)
        for up_name, info in up_dict.items():
            try:
                last_vid = info.get('last_vid')
                
                # é€šè¿‡åç§°æœç´¢æœ€æ–°è§†é¢‘
                results = await get_bilibili_search(up_name, "up")
                if not results:
                    continue
                
                # ç²¾ç¡®åŒ¹é…åç§°
                latest_video = next(
                    (v for v in results 
                     if normalize_name(v['author']) == normalize_name(up_name)),
                    None
                )
                
                if not latest_video:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°è§†é¢‘
                if latest_video['bvid'] != last_vid:
                    # éªŒè¯å‘å¸ƒæ—¶é—´æ˜¯å¦æ™šäºä¸Šæ¬¡æ£€æŸ¥æ—¶é—´
                    last_check_time = datetime.fromisoformat(info['last_check'])
                    video_pub_time = datetime.fromtimestamp(latest_video['pubdate'])
                    
                    if video_pub_time > last_check_time:
                        pub_time = time.strftime("%Y-%m-%d %H:%M", time.localtime(latest_video['pubdate']))
                        
                        # å¤„ç†å›¾ç‰‡URL
                        pic_url = latest_video['pic']
                        if not pic_url.startswith(('http://', 'https://')):
                            pic_url = 'https:' + pic_url
                        proxied_url = f'https://images.weserv.nl/?url={quote(pic_url.replace("https://", "").replace("http://", ""), safe="")}'
                        
                        msg = [
                            f"ğŸ“¢ UPä¸»ã€{up_name}ã€‘å‘å¸ƒäº†æ–°è§†é¢‘ï¼",
                            f"æ ‡é¢˜: {latest_video['title']}",
                            f"[CQ:image,file={proxied_url}]",
                            f"å‘å¸ƒæ—¶é—´: {pub_time}",
                            f"è§†é¢‘é“¾æ¥: https://b23.tv/{latest_video['bvid']}"
                        ]
                        
                        watch_storage.update_last_video(
                            group_id=group_id,
                            up_name=up_name,
                            last_vid=latest_video['bvid']
                        )
                        
                        await bot.send_group_msg(group_id=group_id, message="\n".join(msg))
                
            except Exception as e:
                sv.logger.error(f'ç›‘æ§UPä¸»ã€{up_name}ã€‘å¤±è´¥: {str(e)}')

@sv.on_prefix('æŸ¥è§†é¢‘')
async def search_bilibili_video(bot, ev: CQEvent):
    keyword = ev.message.extract_plain_text().strip()
    if not keyword:
        await bot.send(ev, 'è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼šæŸ¥è§†é¢‘ åŸç¥')
        return
    
    try:
        msg_id = (await bot.send(ev, "ğŸ”ğŸ”ğŸ”ğŸ” æœç´¢ä¸­..."))['message_id']
        results = await get_bilibili_search(keyword, "video")
        
        if not results:
            await bot.finish(ev, f'æœªæ‰¾åˆ°"{keyword}"ç›¸å…³è§†é¢‘')
            return

        reply = ["ğŸ“ºğŸ“ºğŸ“ºğŸ“º æœç´¢ç»“æœï¼ˆæœ€å¤š5ä¸ªï¼‰ï¼š", "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"]
        for i, video in enumerate(results, 1):
            clean_title = re.sub(r'<[^>]+>', '', video['title'])
            pub_time = time.strftime("%Y-%m-%d", time.localtime(video['pubdate']))
            
            # å¤„ç†å›¾ç‰‡URL
            pic_url = video['pic']
            if not pic_url.startswith(('http://', 'https://')):
                pic_url = 'https:' + pic_url
            proxied_url = f'https://images.weserv.nl/?url={quote(pic_url.replace("https://", "").replace("http://", ""), safe="")}'
            
            reply.extend([
                f"{i}. {clean_title}",
                f"[CQ:image,file={proxied_url}]",  # å›¾ç‰‡æ”¾åœ¨æ ‡é¢˜ä¸‹æ–¹
                f"   ğŸ“…ğŸ“…ğŸ“…ğŸ“… {pub_time} | ğŸ‘¤ğŸ‘¤ğŸ‘¤ğŸ‘¤ {video['author']}",
                f"   ğŸ”—ğŸ”—ğŸ”—ğŸ”— https://b23.tv/{video['bvid']}",
                "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            ])
        
        await safe_send(bot, ev, "\n".join(reply))
    except Exception as e:
        await bot.send(ev, f'æœç´¢å¤±è´¥: {str(e)}')

@sv.scheduled_job('interval', minutes=3)
async def clear_cache():
    global search_cache
    expired_keys = [k for k, (_, t) in search_cache.items() 
                   if datetime.now() - t > timedelta(minutes=CACHE_EXPIRE_MINUTES)]
    for k in expired_keys:
        del search_cache[k]
